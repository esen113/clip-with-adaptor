CLIP with Adapter

Project Overview

This project enhances OpenAI's CLIP (Contrastive Language-Image Pretraining) model by incorporating adapter layers. Adapters help fine-tune large-scale pre-trained models efficiently while keeping the original model parameters mostly frozen.


By introducing adapter layers, we enable parameter-efficient fine-tuning (PEFT), reducing computational costs while adapting CLIP to new tasks.

