{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load data from flickr8k"
      ],
      "metadata": {
        "id": "NS6LVwroaH-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeutGyqLNe7X",
        "outputId": "da7c6716-f17c-46d0-949b-5528f7d201a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.04G/1.04G [00:47<00:00, 23.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "print(\"list:\", os.listdir(path))\n",
        "\n",
        "images_dir = os.path.join(path, \"Images\")\n",
        "print(\"Images:\", os.listdir(images_dir)[:10])\n",
        "\n",
        "captions_file = os.path.join(path, \"captions.txt\")\n",
        "with open(captions_file, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "    print(\"captions.txt top 3:\", lines[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqa1e5Dw3XEE",
        "outputId": "1f0692e9-adb2-418a-dcdf-4ffe3366ad8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list: ['captions.txt', 'Images']\n",
            "Images: ['3333017828_b930b9d41b.jpg', '3248408149_41a8dd90d3.jpg', '255741044_1102982213.jpg', '464527562_a18f095225.jpg', '315436114_6d386b8c36.jpg', '3581818450_546c89ca38.jpg', '2904997007_23d4b94101.jpg', '2675397335_1dcdbd12f5.jpg', '3534824784_7133119316.jpg', '2766726291_b83eb5d315.jpg']\n",
            "captions.txt top 3: ['image,caption\\n', '1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\\n', '1000268201_693b08cb0e.jpg,A girl going into a wooden building .\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# change the data to the way can be use in CLIP"
      ],
      "metadata": {
        "id": "ktVAIaN_aTSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class Flickr8kCLIPDataset(Dataset):\n",
        "    def __init__(self, images_dir, captions_file, transform=None, tokenizer=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.samples = []  # save (img_path, caption)\n",
        "\n",
        "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # drop the fisrt line : \"image,caption\"\n",
        "        for line in lines[1:]:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(',', 1)\n",
        "            if len(parts) == 2:\n",
        "                filename = parts[0].strip()\n",
        "                caption = parts[1].strip()\n",
        "                img_path = os.path.join(self.images_dir, filename)\n",
        "                self.samples.append((img_path, caption))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, caption = self.samples[idx]\n",
        "\n",
        "        # 1) load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # 2) tokenize text\n",
        "        if self.tokenizer is not None:\n",
        "\n",
        "            text_enc = self.tokenizer(\n",
        "                caption,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=77,  # CLIP setting\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            # squeeze first dimentio => [batch=1, seq_len] -> [seq_len]\n",
        "            text_enc = {k: v.squeeze(0) for k, v in text_enc.items()}\n",
        "        else:\n",
        "            # or keep the original\n",
        "            text_enc = caption\n",
        "\n",
        "        return image, text_enc"
      ],
      "metadata": {
        "id": "d_fn57pS3P3L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "#use BertTokenizer so it work well with Bert pretain model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize((224,224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "])\n",
        "\n",
        "dataset = Flickr8kCLIPDataset(images_dir, captions_file, transform=transform, tokenizer=tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# check the shape\n",
        "images, text_enc = next(iter(dataloader))\n",
        "print(\"images.shape:\", images.shape)  # [4, 3, 224, 224]\n",
        "print(\"text_enc['input_ids'].shape:\", text_enc[\"input_ids\"].shape)   # [4, 77]\n",
        "print(\"text_enc['attention_mask'].shape:\", text_enc[\"attention_mask\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMvivePF3gij",
        "outputId": "e981b476-9ff2-4fa9-bd1e-eb3e0504e2c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images.shape: torch.Size([100, 3, 224, 224])\n",
            "text_enc['input_ids'].shape: torch.Size([100, 77])\n",
            "text_enc['attention_mask'].shape: torch.Size([100, 77])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP structure"
      ],
      "metadata": {
        "id": "FDHFvPtnaskL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "adapter"
      ],
      "metadata": {
        "id": "uEA86Ndyf1pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#set a simple Adapter, simple mlp and res net\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, hidden_dim, adapter_dim=64):\n",
        "        super().__init__()\n",
        "        self.down = nn.Linear(hidden_dim, adapter_dim)\n",
        "        self.up = nn.Linear(adapter_dim, hidden_dim)# give it same input shape as out put shape so it work well with pretrain model\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.down(x)\n",
        "        z = self.act(z)\n",
        "        z = self.up(z)\n",
        "        #res net\n",
        "        return x + z"
      ],
      "metadata": {
        "id": "AIDsPjhk3QU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "#mix adaptor and bert\n",
        "def add_adapters_to_bert(bert_model, adapter_dim=64):\n",
        "    # bert_model => BertModel\n",
        "    for layer in bert_model.encoder.layer:\n",
        "        hidden_dim = layer.output.dense.out_features# get hidden_dim for adapter\n",
        "        adapter = Adapter(hidden_dim, adapter_dim)\n",
        "        layer.adapter = adapter\n",
        "\n",
        "        old_forward = layer.forward\n",
        "\n",
        "        def new_forward(self, hidden_states, *args, **kwargs):\n",
        "            outputs = old_forward(hidden_states, *args, **kwargs)\n",
        "            # we just want hidden_states here\n",
        "            last_hidden = outputs[0]\n",
        "            last_hidden = self.adapter(last_hidden)# add adapter after last_hidden, and get the result of thing go through the adapter\n",
        "\n",
        "            outputs = (last_hidden,) + outputs[1:]# relace only the last_hidden with last_hidden + adapter\n",
        "            return outputs\n",
        "\n",
        "        layer.forward = types.MethodType(new_forward, layer)\n",
        "    return bert_model"
      ],
      "metadata": {
        "id": "3ZLr2st33QzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_adapters_to_vit(vit_model, adapter_dim=64):\n",
        "\n",
        "    for layer in vit_model.encoder.layer:\n",
        "\n",
        "        hidden_dim = layer.output.dense.out_features\n",
        "        adapter = Adapter(hidden_dim, adapter_dim)\n",
        "        layer.adapter = adapter\n",
        "\n",
        "        old_forward = layer.forward\n",
        "\n",
        "        def new_forward(self, hidden_states, *args, **kwargs):# same as bert\n",
        "\n",
        "            outputs = old_forward(hidden_states, *args, **kwargs)\n",
        "            layer_output = outputs[0]  # hidden_states\n",
        "            # go though adapter\n",
        "            layer_output = self.adapter(layer_output)\n",
        "            # put back tuple\n",
        "            outputs = (layer_output,) + outputs[1:]\n",
        "            return outputs\n",
        "\n",
        "        layer.forward = types.MethodType(new_forward, layer)\n",
        "\n",
        "    return vit_model"
      ],
      "metadata": {
        "id": "U25JX-5J3RUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "#  ViT pretrain\n",
        "image_model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "vit_model = AutoModel.from_pretrained(image_model_name)  # ViTModel\n",
        "\n",
        "#  BERT pretrain\n",
        "text_model_name = \"bert-base-uncased\"\n",
        "bert_model = AutoModel.from_pretrained(text_model_name)  # BertModel\n",
        "\n",
        "# freeze all pretrain\n",
        "for param in vit_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# add Adapter\n",
        "vit_model = add_adapters_to_vit(vit_model, adapter_dim=64)\n",
        "bert_model = add_adapters_to_bert(bert_model, adapter_dim=64)"
      ],
      "metadata": {
        "id": "EbKPjgZh4Ia4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the cls token from vit and bert\n",
        "class FrozenViTWithAdapter(nn.Module):\n",
        "    def __init__(self, vit_model):\n",
        "        super().__init__()\n",
        "        self.vit = vit_model\n",
        "    def forward(self, images):\n",
        "\n",
        "        outputs = self.vit(images)\n",
        "\n",
        "        cls_emb = outputs.last_hidden_state[:, 0, :]#take the cls\n",
        "        return cls_emb\n",
        "\n",
        "class FrozenBertWithAdapter(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        cls_emb = outputs.last_hidden_state[:, 0, :]#take the cls\n",
        "        return cls_emb"
      ],
      "metadata": {
        "id": "si2faDd54Mt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP"
      ],
      "metadata": {
        "id": "8sND_SmjgXZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCLIP(nn.Module):\n",
        "    def __init__(self, vit_model, bert_model, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.image_encoder = FrozenViTWithAdapter(vit_model)\n",
        "        self.text_encoder = FrozenBertWithAdapter(bert_model)\n",
        "\n",
        "        # linear project for each model\n",
        "        self.img_proj = nn.Linear(768, embed_dim)\n",
        "        self.txt_proj = nn.Linear(768, embed_dim)\n",
        "\n",
        "        # logit_scale\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * 1.0)\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask=None):\n",
        "        img_emb = self.image_encoder(images)           # [B, 768]\n",
        "        txt_emb = self.text_encoder(input_ids, attention_mask)  # [B, 768]\n",
        "\n",
        "        # linear project\n",
        "        img_emb = self.img_proj(img_emb)  # [B, embed_dim]\n",
        "        txt_emb = self.txt_proj(txt_emb)  # [B, embed_dim]\n",
        "\n",
        "        # L2 normalize\n",
        "        img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
        "        txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return img_emb, txt_emb, self.logit_scale"
      ],
      "metadata": {
        "id": "G-MUvG9BgjDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contrastive learning"
      ],
      "metadata": {
        "id": "7qbXJaDRgqFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_contrastive_loss(img_emb, txt_emb, logit_scale):\n",
        "    \"\"\"\n",
        "    img_emb: [B, embed_dim]\n",
        "    txt_emb: [B, embed_dim]\n",
        "    \"\"\"\n",
        "    batch_size = img_emb.size(0)\n",
        "    # marix: [B, B]\n",
        "    sim_matrix = img_emb @ txt_emb.t()  # 如果已经 L2 norm，那就是 cosine\n",
        "    # 缩放\n",
        "    sim_matrix = logit_scale.exp() * sim_matrix\n",
        "\n",
        "    labels = torch.arange(batch_size, device=img_emb.device)\n",
        "\n",
        "    import torch.nn.functional as F\n",
        "    loss_img = F.cross_entropy(sim_matrix, labels)\n",
        "    loss_txt = F.cross_entropy(sim_matrix.t(), labels)\n",
        "    loss = (loss_img + loss_txt) / 2.0\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "WJbcYUHA4X--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "TzL0L7qh4fv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, device=\"cuda\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (images, text_enc) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        input_ids = text_enc[\"input_ids\"].to(device)\n",
        "        attention_mask = text_enc[\"attention_mask\"].to(device)\n",
        "\n",
        "        img_emb, txt_emb, logit_scale = model(images, input_ids, attention_mask)\n",
        "        loss = clip_contrastive_loss(img_emb, txt_emb, logit_scale)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "FkdZWvdG4feY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = MyCLIP(vit_model, bert_model, embed_dim=256).to(device)\n",
        "\n",
        "# just train  adapter + projection + temprature\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(trainable_params, lr=1e-4)\n",
        "\n",
        "print(\"trainable para:\", sum(p.numel() for p in trainable_params))\n",
        "\n",
        "for epoch in range(10):\n",
        "    avg_loss = train_one_epoch(model, dataloader, optimizer, device)\n",
        "    print(f\"Epoch {epoch} - loss = {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3DQRFuE9btM",
        "outputId": "9a2a189f-06bc-4da8-8a9d-d8a2beda3b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "可训练参数量: 2772993\n",
            "Epoch 0 - loss = 3.4731\n",
            "Epoch 1 - loss = 3.2319\n",
            "Epoch 2 - loss = 3.0959\n",
            "Epoch 3 - loss = 2.9791\n",
            "Epoch 4 - loss = 2.8731\n",
            "Epoch 5 - loss = 2.7773\n",
            "Epoch 6 - loss = 2.6791\n",
            "Epoch 7 - loss = 2.5935\n",
            "Epoch 8 - loss = 2.5053\n",
            "Epoch 9 - loss = 2.4209\n"
          ]
        }
      ]
    }
  ]
}